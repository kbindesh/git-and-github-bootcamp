==========================================================================
# DOCKER STORAGE
==========================================================================
docker volume create <volume_name>
docker volume create datavolume
docker volume ls
docker volume inspect datavolume

# Mounting a volume on Container
--------------------------------
docker container run -it --name container2 -v datavolume:/data alpine /bin/sh

# Inside the container, we can now create files in the /data folder
/ # cd /data
/ # echo "Some app code" > Code-01.txt
/ # echo "Some more code" > Code-02.txt

Now, Delete the Container with Volume mounted on it
docker container rm container2

Create a new Container and mount the Volume created in Step#1 on it
docker container run --name container4 -it --rm -v datavolume:/app/data \
centos:7 /bin/bash

cd /app/data
ls –l

# As expected, we should see the two files create in step#2
Code-01.txt
Code-02.txt
-------------------------------------------------------
# Sharing Data between containers using Docker Volumes
-------------------------------------------------------
# Create a container and mount a volume on it | Default volume mode: R/W
docker container run -it --name ContainerWithRWVolume -v shared-data:/data .alpine /bin/sh

# Create a new file inside the container (on mounted volume)
/ echo "New file present in docker volume" > /data/test-file.txt

# Now, exit from the container by pressing Ctrl+D or exit

# Create another container, say ContainerWithROVolume and mount a volume in Read Only mode
docker container run -it --name ContainerWithROVolume -v shared-data:/app/data:ro ubuntu:22.04 /bin/bash

# Check in the container if you can see the file created in the ContainerWithRWVolume container
ls -l /app/data

# Now, try to create a new file on the mounted volume
/ echo "New file for read only volume" > /app/data/test-file-2.txt

# It will fail with the following message:
bash: /app/data/test-file-2.txt: Read-only file system

==========================================================================
# DOCKER NETWORKING
==========================================================================
USING BRIDGE NETWORKS - bridge
--------------------------------------------------------------------------
docker network ls
docker network inspect bridge
docker network create --driver bridge sample-net
docker network create --driver bridge --subnet "10.1.0.0/16" testnet
docker network inspect sample-net | grep Subnet
docker network rm sample-net
docker network prune --force
--------------------------------------------------------------------------

// Create a network without specifying network - for bridge n/w
docker container run --name c1 -it --rm alpine:latest /bin/sh
docker container inspect c1
docker container exec c1 /bin/sh
ip addr
ip addr show eth0 - we can also see what MAC address and what IP have been associated with this container network namespace by Docker.
ip route

docker container run --name c2 -d --rm alpine:latest ping 127.0.0.1
docker container inspect --format "{{.NetworkSettings.IPAddress}}" c2


# Now, let’s create two additional containers, c3 and c4, and attach them to sample-net, which we
created earlier. For this, we’ll use the --network parameter

docker container run --name c3 --rm -d \
--network sample-net \
alpine:latest ping 127.0.0.1

docker container run --name c4 --rm -d \
--network sample-net \
alpine:latest ping 127.0.0.1

docker network inspect sample-net

# whether the c3 and c4 containers can freely communicate with each other?
- To demonstrate that this is indeed the case, we can exec into the c3 container:

$ docker container exec -it c3 /bin/sh
/ # ping c4

Instead of the container name, here, we use c4’s IP address:
/ # ping 172.20.0.3

We should get this output:
PING c4 (172.20.0.3): 56 data bytes
64 bytes from 172.20.0.3: seq=0 ttl=64 time=3.092 ms
64 bytes from 172.20.0.3: seq=1 ttl=64 time=0.481 ms
--------------------------------------------------------------------------
docker network inspect bridge
--------------------------------------------------------------------------
1) Creating Networks
   - To create a new network, use the docker network create command. 
   - You can specify the driver to use, such as bridge or host, by setting the "-d" flag. 
   - By default bridge network will be created if you omit the flag.

docker network create app-network -d bridge

2) Connecting Containers to Networks
   - You can attach new containers to a network by setting the --network flag with your docker run command.

docker run -it --rm --name bincontainer --network app-network busybox:latest

3) Next, start another container, this time without the --network flag (will go in default bridge network):
docker run -it --rm --name container2 busybox:latest

4) Now try communicating between the two containers, using their names:
# get inside container1
/ # ping container2
ping: bad address 'container2'

- The containers aren’t in the same network, so they can’t directly communicate with each other.

5) Join container2 to the same network as container1

docker network connect app-network container2

- The containers now share a network, which allows them to discover each other:
# in container1
/ # ping container2
PING container2 (172.22.0.3): 56 data bytes
64 bytes from 172.22.0.3: seq=0 ttl=64 time=4.205 ms

----------------------------------------------------------------------------
# USING HOST NETWORKS
----------------------------------------------------------------------------
Example-01
-------------

- You can enable host networking for a container by connecting it to the built-in host network:

docker run -d --name nginx --network host nginx:latest

- NGINX listens on port 80 by default. 
- Because the container's using a host network, you can access your NGINX server on your host's localhost:80 outside the container, even though no ports have been explicitly specified.

- To test: curl localhost:80

Example-02
----------
# Run an Alpine container and attach it to the host network
$ docker container run --rm -it --network host alpine:latest /bin/sh
/ # ip addr show eth0

[can see that 192.168.65.3 is the IP address that the host has been assigned and
that the MAC address shown here also corresponds to that of the host.]

We can also inspect the routes:
/ # ip route

----------------------------------------------------------------------------
# USING NULL NETWORKS
----------------------------------------------------------------------------
$ docker container run --rm -it --network none alpine:latest /bin/sh

/ # ip addr show eth0
ip: can't find device 'eth0'

/ # ip route
This returns nothing.
----------------------------------------------------------------------------
REMOVING CONTAINERS FROM THE NETWORKS
----------------------------------------------------------------------------
docker network disconnect demo-network container2

----------------------------------------------------------------------------
MANAGING NETWORKS
----------------------------------------------------------------------------
- docker network ls
- docker network rm <network-name> - To delete a network, disconnect or stop all the Docker containers that are assciated with it.
- docker network prune - automatically delete all unused networks using the network prune

----------------------------------------------------------------------------
DISABLING NETWORKS - none
----------------------------------------------------------------------------
- When a container's networking is disabled, it will have no connectivity available – either to other containers, or your wider network. 
- Disable networking by attaching your container to the none network.
- This lets you easily sandbox unknown services.

docker run -it --rm --network none busybox:latest

============================================================================
CONFIGURING DOCKER
============================================================================

docker container run --rm -it alpine /bin/sh
/ # export

This should produce the following output:
export HOME='/root'
export HOSTNAME='91250b722bc3'
export PATH='/usr/local/sbin:/usr/local/bin:...'
export PWD='/'
export SHLVL='1'
export TERM='xterm'

# Defining environment variables for containers
------------------------------------------------

docker container run --rm -it --env LOG_DIR=/var/log/my-log alpine /bin/sh
/ # export | grep LOG_DIR

The output should be as follows: LOG_DIR='/var/log/my-log'

# Define multiple environment variable
- We just need to repeat the --env (or -e) parameter

docker container run --rm -it \
--env LOG_DIR=/var/log/my-log \
--env MAX_LOG_FILES=5 \
--env MAX_LOG_SIZE=1G \
alpine /bin/sh

/ # export | grep LOG

We will see the following:
export LOG_DIR='/var/log/my-log'
export MAX_LOG_FILES='5'
export MAX_LOG_SIZE='1G'

============================================================================
DOCKER IMAGES
============================================================================

# Flask App Dockerfile
# Which base image to use?
FROM python:3.11-alpine

# Add metadata for the image
LABEL maintainer="KBindesh"

# Set the working directory for your app
WORKDIR /app

# Copy the app source code to working directory
COPY . .

# Install the dependencies | Run the command while building the image
RUN pip install -r requirements.txt

# Mention the port on which app container will be listening
# EXPOSE 8080

# ENTRYPOINT ["python"]

# Command to run when container starts
CMD ["python", "src/app.py"]
-------------------------------------------------------------------
docker build -t image1 .
docker run -it --name container -p 4000:8080 image1

docker run --rm -it --name test alpine /bin/sh
apk add curl
curl http://172.17.0.2:8080

-----------------------------
- By default, the docker run command does not attach the standard input stream (STDIN) of the process within the container to the host terminal. 
- However, it does connect the standard output (STDOUT) and standard error (STDERR) streams. - In other words, we’ll only see the output being printed without a way to send any input to it.
- If the main process of the container is expecting input, running it without attaching the STDIN will cause it to exit immediately.


# Running a Process With Input Prompt Without Attaching STDIN

docker run ubuntu passwd root
New password: Password change has been aborted.
passwd: Authentication token manipulation error
passwd: password unchanged


In this case, we see that the command now waits for our input.

When we pass the -i option, the docker run command attaches the input device to the main process within the container. Specifically, the input device it takes is the input device to the docker run command. Note the term “input device” here because the input device is not necessarily the terminal.

------------------------------------------------------------------
# Attaching Output Pipe to STDIN

We can also attach the output pipe to the STDIN of the process within the Docker container. For instance, let’s pipe the output of the echo command to a cat process in the container:

$ echo "This is a piped input" | docker run -i ubuntu cat

-------------------------------------------------------------------
# Allocating the Pseudo-Terminal With the -t Option

- From the official documentation, Docker states that the -t option will “allocate a pseudo-TTY” to the process inside the container. 
- TTY stands for Teletype and can be interpreted as a device that offers basic input-output. 
- The reason it’s a pseudo-TTY is that there’s no physical teletype needed, and it’s emulated using a combination of display driver and keyboard driver.

- For the sake of this article, it’s sufficient to think of a pseudo-TTY as a terminal console we’re using for running commands and reading output in Linux.

------------------------------------------------------------------------
# Missing Functionality Without the -t Option

- Without passing the -t option to docker run for the interactive mode, not all of the terminal-specific functionality will be working.

For example, most programs that prompt the user for a password will turn off the echo of the input. This allows the password to be hidden from the console.

Let’s run the passwd command with the -i option and complete the prompt:
$ docker run -i ubuntu passwd root 
New password: thisisanewpassword 
Retype new password: thisisanewpassword 
passwd: password updated successfully

Under typical usage, we know that passwd‘s password prompt does not display our password input. The way passwd achieves this is by turning off the echo option of the terminal. Since we started this process without allocating a pseudo-TTY, the echo-off functionality of the terminal is not taking effect here.

--------------------------------------------------------------------------
# Enabling Complete Terminal Functionality Using the -t Option

To enable complete terminal functionality, including the echo-off option, we can pass the -t option along with -i:

$ docker run -i -t ubuntu passwd root
New password: 
Retype new password: 
passwd: password updated successfully


Now, the echo-off option on the passwd command is working correctly under the pseudo-TTY environment.


-----------------------------------------------------------------------------
#  “Input device is not a TTY” Error

If we specify -t and then attach an output pipe to the STDIN of the container’s process, the docker run command will complain that the “input device is not a TTY”. This is because when the -t option is present, docker run will check if the input device is a TTY-like device. When it sees the input device is a pipe file, it’ll exit with an error:

$ echo "this is a pipe input" | docker run -i -t ubuntu cat
the input device is not a TTY
$ echo $?
1

------------------------------------------------------------------------------
SUMMARY
-------
- We’ve learned that the -i option of the docker run command attaches the STDIN of the container to the host process. 
- Then, we’ve also seen how the -t option enables complete terminal functionality such as echo-off for password masking. 
- Then, we also simulated the “input device is not a TTY” error by piping the pipe files and specifying the -t option.


=========================================
DOCKER COMPOSE
=========================================
- monolithic vs distributed application architecture

- Single host networking

- Network firewalling

- Bridge Networks

- Imperative vs Declarative orchestration of containers

- Introduction to Docker Compose

- Setup Docker Compose

- List Docker Compose commands


----------------
# Example-01
----------------

- Running a multi-service app
  1. Postgress SQL (5432)
  2. SQLClient (5050)


services:
  db:
    image: postgres:alpine
    environment:
      - POSTGRES_USER=dockeruser
      - POSTGRES_PASSWORD=dockerpass
      - POSTGRES_DB=pets
    volumes:
      - pg-data:/var/lib/postgresql/data
      - ./db:/docker-entrypoint-initdb.d

  pgadmin:
    image: dpage/pgadmin4
    ports:
      - "5050:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@acme.com
      PGADMIN_DEFAULT_PASSWORD: admin
    volumes:
      - pgadmin-data:/var/lib/pgadmin

volumes:
  pg-data:
  pgadmin-data:

- Create a folder called db in the step1 folder and add a file called init-db.sql to it with the following content:

```
CREATE TABLE images
(
  imageid serial UNIQUE PRIMARY KEY,
  description character varying(10485760) NOT NULL,
  url character varying(255) NOT NULL
);

ALTER TABLE images
  OWNER TO dockeruser;
ALTER ROLE dockeruser CONNECTION LIMIT -1;

-- add image data
INSERT INTO images (description, url) VALUES('vulture on tree', 'images/vulture.png');
INSERT INTO images (description, url) VALUES('female lions', 'images/3-female-lions.png');
INSERT INTO images (description, url) VALUES('antelopes', 'images/antelopes.png');
INSERT INTO images (description, url) VALUES('birds', 'images/birds.png');
INSERT INTO images (description, url) VALUES('buffalo', 'images/buffalo.png');
INSERT INTO images (description, url) VALUES('cheetah', 'images/cheetah.png');
INSERT INTO images (description, url) VALUES('elephants', 'images/elephants.png');
INSERT INTO images (description, url) VALUES('jackal', 'images/jackal.png');
INSERT INTO images (description, url) VALUES('giraffe', 'images/giraffe.png');
INSERT INTO images (description, url) VALUES('hippos', 'images/hippos.png');
INSERT INTO images (description, url) VALUES('male lion', 'images/male-lion.png');
INSERT INTO images (description, url) VALUES('zebra', 'images/zebra.png');
```

# Deploy the application
- docker compose up -d
- navigate to http://dockerhost:5050

# Terminate the application
docker compose down -v

# Scale the application 
docker compose up --scale web=3
[If we do this, we are in for a surprise. The output will look as in the following screenshot:]

The second and third instances of the web service fail to start. The error message tells us why we
cannot use the same host port, 3000, more than once. When instances 2 and 3 try to start, Docker
realizes that port 3000 is already taken by the first instance.

- SOLUTION - If, in the ports section of the Docker Compose file, we only specify the container port and leave out
the host port, then Docker automatically selects an ephemeral port. Let’s do exactly this

  - Then, we modify the docker-compose.yml file. The port mapping of the web service
originally looks like this:

```
ports:
– 3000
```

- Now, we can start the application again and scale it up immediately after that

```
$ docker compose up -d
$ docker compose up -d --scale web=3
```

# Build the application

- docker compose build
- docker-compose -f docker-compose.dev.yml build
- docker compose up --scale web=3
- docker compose up -d --scale web=3

- docker login -u gnschenker -p <password>
- docker-compose -f docker-compose.dev.yml push


Example-02
--------------
services:
  rabbitmq:
    image: rabbitmq:3.13.2-management-alpine
    container_name: 'rabbitmq'
    restart: always
    environment:
      - "RABBITMQ_DEFAULT_USER=username"
      - "RABBITMQ_DEFAULT_PASS=password"
    ports:
      - 15672:15672
      - 5672:5672
    healthcheck:
      test: ["CMD", "rabbitmqctl", "status"]
      interval: 30s
      timeout: 10s
      retries: 5
    volumes:
      - ./rabbitmq_enabled_plugins:/etc/rabbitmq/enabled_plugins
    networks:
      - backend_services
  order-service:
    build: src/order-service
    container_name: 'order-service'
    restart: always
    ports:
      - 3000:3000
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "-q", "http://order-service:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      - ORDER_QUEUE_HOSTNAME=rabbitmq
      - ORDER_QUEUE_PORT=5672
      - ORDER_QUEUE_USERNAME=username
      - ORDER_QUEUE_PASSWORD=password
      - ORDER_QUEUE_NAME=orders
      - ORDER_QUEUE_RECONNECT_LIMIT=3
    networks:
      - backend_services
    depends_on:
      rabbitmq:
        condition: service_healthy
  product-service:
    build: src/product-service
    container_name: 'product-service'
    restart: always
    ports:
      - 3002:3002
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "-q", "http://product-service:3002/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      - AI_SERVICE_URL=http://ai-service:5001/
    networks:
      - backend_services
  store-front:
    build: src/store-front
    container_name: 'store-front'
    restart: always
    ports:
      - 8080:8080
    healthcheck:
      test: ["CMD", "wget", "-O", "/dev/null", "-q", "http://store-front:80/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    environment:
      - VUE_APP_PRODUCT_SERVICE_URL=http://product-service:3002/
      - VUE_APP_ORDER_SERVICE_URL=http://order-service:3000/
    networks:
      - backend_services
    depends_on:
      - product-service
      - order-service
networks:
  backend_services:
    driver: bridge

-----------------------------------------
docker compose -f docker-compose-quickstart.yml up -d

docker images

docker ps

http://localhost:8080

docker compose down

======================================================================
DOCKER SWARM
-=====================================================================

docker swarm init

- When you initialize the docker swarm mode, following commands gets activated:
  - docker swarm - ca, init, join join-token, leave, update
  - docker node - demote, promote, rm, ls, inspect
  - docker service - create, inspect, logs, ls, ps, rm, rollback, scale, update
  - docker stack - config, deploy, ls, ps, rm, services
  - docker secret - create, inspect, ls, rm


# To join worker node to the swarm cluster
docker swarm join --token SWMTKN

# Swarm Intro and Creating a 3-Node Swarm Cluster

## Create Your First Service and Scale it Locally

docker info

docker swarm init

docker node ls

docker node --help

docker swarm --help

docker service --help

docker service create alpine ping 8.8.8.8

# List docker compose services
docker service ls

# List all the containers (replicas) associated with a service
docker service ps <service_name>

# List containers
docker container ls

# Scale a service
docker service update --help
docker service update <service_name> --replicas 3

docker service ls

docker service ps <service_name>

docker container ls


docker container rm -f frosty_newton.1.TAB COMPLETION
[Error response from daemon: cannot remove container | won't let you delete]

docker container stop <container_name>
[Container will be restarted again]

docker service ls

docker service ps <service_name>

docker service rm <service_name>

docker service ls

docker container ls

## Creating a 3-Node Swarm Cluster

http://play-with-docker.com

docker info

docker-machine

docker-machine create node1

docker-machine ssh node1

docker-machine env node1

docker info

http://get.docker.com

docker swarm init

docker swarm init --advertise-addr TAB COMPLETION

docker node ls

docker node update --role manager node2

docker node ls

docker swarm join-token manager

docker node ls

docker service create --replicas 3 alpine ping 8.8.8.8

docker service ls

docker node ps

docker node ps node2

docker service ps sleepy_brown

## Scaling Out with Overlay Networking

docker network create --driver overlay mydrupal

docker network ls

docker service create --name psql --netowrk mydrupal -e POSTGRES_PASSWORD=mypass postgres

docker service ls

docker service ps psql

docker container logs psql TAB COMPLETION

docker service create --name drupal --network mydrupal -p 80:80 drupal

docker service ls

watch docker service ls

docker service ps drupal

docker service inspect drupal

======================================================================
# Swarm Basic Features and How to Use Them In Your Workflow

## Scaling Out with Overlay Networking

docker network create --driver overlay mydrupal

docker network ls

docker service create --name psql --network mydrupal -e POSTGRES_PASSWORD=mypass postgres

docker service ls

docker service ps psql

docker container logs psql TAB COMPLETION

docker service create --name drupal --network mydrupal -p 80:80 drupal

docker service ls

watch docker service ls

docker service ps drupal

docker service inspect drupal

## Scaling Out with Routing Mesh

docker service create --name search --replicas 3 -p 9200:9200 elasticsearch:2

docker service ps search

## Assignment Answers: Create a Multi-Service Multi-Node Web App

docker node ls

docker service ls

docker network create -d overlay backend

docker network create -d overlay frontend

docker service create --name vote -p 80:80 --network frontend -- replica 2 COPY IMAGE

docker service create --name redis --network frontend --replica 1 redis:3.2

docker service create --name worker --network frontend --network backend COPY IMAGE

docker service create --name db --network backend COPY MOUNT INFO

docker service create --name result --network backend -p 5001:80 COPY INFO

docker service ls

docker service ps result

docker service ps redis

docker service ps db

docker service ps vote

docker service ps worker

cat /etc/docker/

docker service logs worker

docker service ps worker

## Swarm Stacks and Production Grade Compose

docker stack deploy -c example-voting-app-stack.yml voteapp

docker stack

docker stack ls

docker stack ps voteapp

docker container ls

docker stack services voteapp

docker stack ps voteapp

docker network ls

docker stack deploy -c example-voting-app-stack.yml voteapp

## Using Secrets in Swarm Services

docker secret create psql_usr psql_usr.txt

echo "myDBpassWORD" | docker secret create psql_pass - TAB COMPLETION

docker secret ls

docker secret inspect psql_usr

docker service create --name psql --secret psql_user --secret psql_pass -e POSTGRES_PASSWORD_FILE=/run/secrets/psql_pass -e POSTGRES_USER_FILE=/run/secrets/psql_user postgres

docker service ps psql

docker exec -it psql.1.CONTAINER NAME bash

docker logs TAB COMPLETION

docker service ps psql

docker service update --secret-rm

## Using Secrets with Swarm Stacks

vim docker-compose.yml

docker stack deploy -c docker-compose.yml mydb

docker secret ls

docker stack rm mydb

## Assignment Answers: Create A Stack with Secrets and Deploy

vim docker-compose.yml

docker stack deploy - c docker-compose.yml drupal

echo STRING |docker secret create psql-ps - VALUE

docker stack deploy -c docker-compose.yml drupal

docker stack ps drupal



======================================================================
QUERIES
-=====================================================================
By default (using the default docker builder), you can only build images for a single platform at a time and for the docker host architecture only.

# To know your docker host architecture
docker system info 

- Docker also allows you to build the images for
  1. Target architecture other than the architecture of the machine where the build runs (e.g build the image for ARM64 on the machine with AMD64 architecture)
  2. multiple platforms, which means that a single image may contain variants for different architectures, e.g. ARM64 and AMD64 (X86_64).


# Single-platform image

- QEMU

# Verify that arm architecture was properly added
docker buildx ls

# Build an image for the target architecture by specifying the –platform flag
docker build --platform linux/amd64 -t <IMAGE-URI> -f Dockerfile .

# You can also use the buildx plugin
docker buildx build --platform linux/amd64 -t <IMAGE-URI> -f Dockerfile .

------------------------------------------------------------------------------------------------------
# Multi-platform image

- To build for multiple platforms at once, create a new builder that uses the docker-container driver. When using docker-container driver with buildx, –-platform flag can accept multiple values as an input separated by a comma. With multiple values the result will be built for all of the specified platforms and joined together into a single manifest list.

# Build the image with buildx, passing the list of architectures (linux/amd64 and linux/arm64) to build for:
docker buildx build --platform linux/amd64,linux/arm64 --push -t <IMAGE-URI> -f Dockerfile .

docker buildx imagetools inspect <IMAGE-URI>

