INTRODUCTION TO CONTAINER ORCHESTRATION
========================================
VARIOUS CONTAINER ORCHESTRATION TOOLS - K8s, Docker Swarm, ECS, Nomad, Apache Mesos
====================================================================================
WHAT IS K8S?
===============
VARIOUS K8S DISTRIBUTIONS
=========================
K8s ARCHITECTURE
=================
K8s API OBJECTS - PODs
=================
IMPERATIVE vs DECLARATIVE APPROACHES TO MANAGE K8s Resources
============================================================
K8S WORKLOAD SCHEDULING
==========================
UNDERSTANDING PODS
==========================


DEPLOY 1ST APP (POD)
=====================
Imperative
------------
kubectl run nginx --image nginx:latest
kubectl run bin-nginx-pod --image=nginx:alpine

# Example-02: Create a Pod with busybox image
kubectl run busybox --rm -it --image=busybox /bin/sh

# You can also deploy a nginx image and then export the YAML definition
kubectl run nginx --image=nginx --dry-run -o yaml > pod-sample.yaml

$ kubectl get pods
$ kubectl get pods -o wide
$ kubectl describe pod <pod_name>

Declarative
-----------
- Install VS Code - K8s extension
- Syntax of a manifest

Lab-01: Deploy nginx pod (single container)
---------------------------------------------
apiVersion: apps/v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx
      image: nginx:latest

kubectl apply -f pod.yml

Lab-01: Deploy nginx pod (multi container pod)
---------------------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: multi-app-pod
  labels:
    app: multi-app
spec:
  containers:
  - name: nginx
    image: nginx
    ports:
    - containerPort: 80
  - name: busybox-sidecar
    image: busybox
    command: ['sh', '-c', 'while true; do sleep 3600; done;']

Pod with resource limits
---------------------------------------------------

apiVersion: v1
kind: Pod
metadata:
  name: frontend
spec:
  containers:
    - name: app
      image: images.my-company.example/app:v4
      resources:
        requests:
          memory: "64Mi"
          cpu: "250m"
        limits:
          memory: "128Mi"
          cpu: "500m"

---------------------------------------------------
Understanding an init container
---------------------------------------------------
An init container is configured in a pod to execute before the container host starts. It is specified
inside an initContainers section, as in the following example. You can configure multiple init
containers too, which will allow each init container to complete one at a time in sequential order:

apiVersion: v1
kind: Pod
metadata:
  name: melon-pod
  labels:
    app: melonapp
spec:
  containers:
  - name: melonapp-container
    image: busybox:latest
    command: ['sh', '-c', 'echo The melonapp is running! &&
    sleep 3600']
  initContainers:
  - name: init-melonservice
    image: busybox:latest
    command: ['sh', '-c', 'until nslookup melonservice; do echo
    waiting for melonservice; sleep 2; done;']

In the case that any of the init containers fail to complete, Kubernetes will restart the pod repeatedly until
the init container succeeds.

=====================================================

=====================================================
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

# Pod Lifecycle phases - Pending, Running, Succeeded, Failed, Unknown
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

# Container restart policy 
The spec of a Pod has a restartPolicy field with possible values Always, OnFailure, and Never. The default value is Always.

=====================================================
Deploying and managing applications
=====================================================
# Replicasets
-------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
name: frontend
labels:
app: melonapp-rs
spec:
replicas: 3
selector:
matchLabels:
app: melonapp-rs
template:
metadata:
labels:
app: melonapp-rs
spec:
containers:
- name: nginx
image: nginx

kubectl get replicaset

Once the ReplicaSet is deployed, update the number of ReplicaSets by using the following command:
kubectl scale replicaset frontend --replicas=6

Alternatively, you can specify it in a YAML definition with the following command:
kubectl scale --replicas=6 -f replicas.yaml

kubectl delete replicaset frontend

=========================================
Workload scheduling
=========================================
Understanding namespaces

kubectl get namespaces

When you define a pod or any namespaced Kubernetes object, you can specify the namespace in the
YAML definition as follows:

apiVersion: v1
kind: Pod
metadata:
  name: k8s-ns-pod
  namespace: k8s-ns
  labels:
    app: k8sapp
spec:
  containers:
  - name: k8sapp-container
    image: busybox
    command: ['sh', '-c', 'echo Salut K8S! && sleep 3600']

# If you create that pod and specify the namespace that the pod belongs to, you can add the -n flag
when querying this pod using the kubectl get pods command:

kubectl get pods -n k8s-ns

# Similarly, if the pod has been created in that namespace, you can use the following command to
check it out:
kubectl describe pod k8s-ms-pod -n k8s-ns

# In the case that the pods are not in the default namespace, you don’t have to specify the namespace
option anymore. In the following example, you want to set a namespace named dev, and then use
the kubectl get command without the -n flag:

kubectl config set-context &(kubectl config current-context)
--namespace=dev

====================================================
Labels, node selectors, and annotations
====================================================
- Labels, selectors, and annotations are useful notions when it comes to workload scheduling. Labels are key-value pairs attached to Kubernetes objects that can be listed in the metadata.labels section of an object descriptor. 
- Selectors are used for identifying and selecting a group of objects using their labels. 

- See the following examples of some quality-based selectors:

kubectl get pods -l app=my-app
kubectl get pods -l environment=production

# When it comes to inequality, you can use the following:
kubectl get pods -l environment!=production

# You can start by labeling the worker nodes using the following command:
kubectl label node cloudmelonplayground env=dev

========================================================
SERVICES
========================================================
Kubernetes treats Pods as ephemeral objects and deletes them when any of the following
events occur:
• Scale-down operations
• Rolling updates
• Rollbacks
• Failures

This means they’re unreliable, and apps can’t rely on them being there to respond to
requests. Fortunately, Kubernetes has a solution — Service objects sit in front of one or
more identical Pods and expose them via a reliable DNS name, IP address, and port.

- Every Service has a front end and a back end. 
- The front end includes a DNS name, IP address, and network port that Kubernetes guarantees will never change. 
- The back end is a label selector that sends traffic to healthy Pods with matching labels.

Example for exmplaining service

apiVersion: apps/v1
kind: Deployment
metadata:
  name: bin-2024
spec:
  replicas: 10
  <Snip>
  template:
    metadata:
      labels:
      project: tkb <<==== Create Pods with these labels
      zone: prod <<==== Create Pods with these labels
    spec:
      containers:
  <Snip>
---
apiVersion: v1
kind: Service
metadata:
  name: tkb
spec:
  ports:
    - port: 8080
  selector:
    project: tkb <<==== Send to Pods with these labels
    zone: prod <<==== Send to Pods with these labels


==================================================================
Service types
Kubernetes has several types of Services for different use cases and requirements. The
major ones are:
• ClusterIP
• NodePort
• LoadBalancer

===================================================================
NodePort
===================================================================
apiVersion: v1
kind: Service
metadata:
  name: skippy <<==== Registered with the internal cluster DNS (ClusterIP)
spec:
  type: NodePort <<==== Service type
  ports:
  - port: 8080 <<==== ClusterIP port
    targetPort: 9000 <<==== Application port in container
    nodePort: 30050 <<==== External port on every cluster node (NodePort)
  selector:
    app: hello-world

kubectl describe svc svc-test
[Endpoints: 10.1.0.200:8080,10.1.0.201:8080,10.1.0.202:808]
====================================================================
Loadbalancer
====================================================================
apiVersion: v1
kind: Service
metadata:
  name: lb <<==== Registered with cluster DNS
spec:
  type: LoadBalancer
  ports:
  - port: 8080 <<==== Load balancer port
    targetPort: 9000 <<==== Application port inside container
  selector:
    project: tkb

kubectl describe svc svc-test
[Endpoints: 10.1.0.200:8080,10.1.0.201:8080,10.1.0.202:8080]


===================================================================
KUBERNETES SECURITY
===================================================================
kubectl get roles -A
kubectl get rolesbindings -A


https://kubernetes.io/docs/reference/access-authn-authz/rbac/

Kubernetes allows you to configure custom roles or use default user-facing roles, including, but not limited to:

1. Cluster-admin: 
 - This “superuser” can perform any action on any resource in a cluster. 
 - You can use this in a ClusterRoleBinding to grant full control over every resource in the cluster (and in all namespaces) or in a RoleBinding to grant full control over every resource in the respective namespace.
2. Admin: 
 - This role permits unlimited read/write access to resources within a namespace. 
 - This role can create roles and role bindings within a particular namespace. It does not permit write access to the namespace itself.
3. Edit: This role grants read/write access within a given Kubernetes namespace. 
   - It cannot view or modify roles or role bindings. 
4. View: This role allows read-only access within a given namespace. 
   - It does not allow viewing or modifying of roles or role bindings. 


# Creating roles for users per user type
===========================================
As an example, let’s say your organization needs roles for three different user types: 
1. infrastructure monitoring team members - you could configure a Role that gives read-only access (using the verbs “get,” “list” and “watch”) to a given namespace.
2. established developers - 
3. cluster admins - And admin could map new developers, who are still getting the hang of things, or anyone else who needs read-only access, to this Role.

# READ-ONLY ROLE for infrastructure monitoring users

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: read-only
  namespace: default
rules:
- apiGroups:
  - ""
  resources: ["*"]
  verbs:
  - get
  - list
  - watch

To apply the Role to a user, you must define a RoleBinding. 
This will give the user access to all resources within the namespace with the permissions defined in the Role configuration above:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: read-only-binding
roleRef:
  kind: Role
  name: read-only #The role name you defined in the Role configuration
  apiGroup: rbac.authorization.k8s.io
subjects:
– kind: User
  name: example #The name of the user to give the role to
  apiGroup: rbac.authorization.k8s.io


# READ-WRITE ROLE FOR DEVELOPERS

Before creating a role, create a dedicated namespace for developers "dev"

Likewise, a Role for developers who need not just the read-access rights from the example above, but also write-access, to a certain namespace (“dev” in this example) would look as follows:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
 name: read-write
 namespace: dev
rules:
- apiGroups:
 - ""
 resources: ["*"]
 verbs:
 - get
 - list
 - watch
 - create
 - update
 - patch
 - delete

The associated RoleBinding would look like:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
 name: read-write-binding
roleRef:
 kind: Role
 name: read-write
 apiGroup: rbac.authorization.k8s.io
subjects:
- kind: User
 name: example
 apiGroup: rbac.authorization.k8s.io


# SUPER-USER ROLE FOR ENTIRE CLUSTER (CLUSTER ROLE)
------------------------------------------------------
For superusers who need admin access to the entire cluster, change the "kind" value to "ClusterRole," which gives the user access to all resources within the cluster, instead of just one namespace:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: superuser
rules:
- apiGroups:
 - ""
 resources: ["*"]
 verbs:
 - get
 - list
 - watch
 - create
 - update
 - patch
 - delete

And in this case, we’ll create a ClusterRoleBinding instead of a RoleBinding:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
 name: superuser-binding
roleRef:
 kind: ClusterRole
 name: superuser
 apiGroup: rbac.authorization.k8s.io
subjects:
- kind: User
 name: superuser
 apiGroup: rbac.authorization.k8s.io


======================================================
SERVICE ACCOUNTS
======================================================

# Create service accounts for each application

- Kubernetes uses service accounts to authenticate and authorize requests by pods to the Kubernetes API server. 
- Kubernetes automatically assigns newly created pods to the “default” service account in your cluster and all applications share this service account. 
- However, this configuration may not be desirable if, 
  for example, you are using some applications for development purposes, and want those applications to use a “dev” service account instead of a default one.

https://www.strongdm.com/blog/kubernetes-rbac-role-based-access-control
https://medium.com/rahasak/kubernetes-role-base-access-control-with-service-account-e4c65e3f25cc
https://kubernetes-tutorial.schoolofdevops.com/configuring_authentication_and_authorization/


========================================================================================================

HPA
----

- Check if metrics server is available or not
kubectl get deployment metrics-server -n kube-system
kubectl get pods -n kube-system | grep metrics-server
Error from server (NotFound): deployments.apps "metrics-server" not found

kubectl top nodes
kubectl top node minikube
error: Metrics API not available


# Install Metrics Server - on EKS
------------------------------------
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# Verify if the metrics server deployment is running 
kubectl get deployment metrics-server -n kube-system

#Test the metrics server is working by displaying resource (CPU/memory) usage of nodes.
kubectl top nodes

# Install Metrics Server - on minikube
------------------------------------
minikube addons list
OR
eksctl get addons --cluster <cluster-name>

minikube addons enable metrics-server



# Deployment an Application and a cluster IP service
------------------------------------------------------ 
kubectl apply -f https://k8s.io/examples/application/php-apache.yaml
kubectl autoscale deployment php-apache --cpu-percent=50 --min=1 --max=10
kubectl get hpa

# Create a load for the web server by running a container.
kubectl run -i \
    --tty load-generator \
    --rm --image=busybox \
    --restart=Never \
    -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://php-apache; done"


[HPA checks metrics and adjusts the number of pods every 15 seconds]
[Metrics-server has a default metric-resolution of 30 seconds, meaning it collects and aggregates metrics every 30 seconds]

kubectl top nodes
kubectl top pods
kubectl top pod --sort-by=cpu
kubectl top pod –-sort-by=memory
kubectl top pod -A --sort-by=memory

Monitoring the resource usage of an Application
------------------------------------------------
kubectl top pod <pod_name>
kubectl top pod php-apache-6487c65df8-55x45 --containers
----------------------------------------------------------
# Monitoring cluster events

# We can get Kubernetes events by using the following command
kubectl get events

kubectl get events --sort-by=.metadata.creationTimestamp

# If you want to collect the events during a deployment, you can run the following command on the side
kubectl get events --watch
------------------------------------------
Understanding liveness, readiness, and startup probes
------------------------------------------
LIVENESS PROBE
---------------
- The liveness probe ensures an application within a container is live and operational based on a specified test. 
- Defining probes correctly can improve Pod resilience and availability.
- A Kubernetes liveness probe is a mechanism used to determine if a container is still running properly. 
- If a liveness probe fails, Kubernetes will restart the container to restore functionality.
- Liveness probes are commonly used to detect application deadlocks, crashes, or unresponsive states that cannot be self-recovered. 
- They are configured using commands, HTTP requests, or TCP socket checks, ensuring Kubernetes can intervene when a container stops responding.

# Types of liveness probes
<refer the diagram>

=> How to define a Kubernetes liveness probe

Example-01: ExecAction handler
----------------

The example below shows the use of the exec command to check if a file exists at the path /usr/share/liveness/html/index.html using the cat command. If no file exists, then the liveness probe will fail, and the container will be restarted.

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-exec
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness:0.1
    ports:
    - containerPort: 8080
    livenessProbe:
      exec:
        command:
        - cat
        - /usr/share/liveness/html/index.html
      initialDelaySeconds: 5
      periodSeconds: 5
```


Example-02 - TCPSocketAction handler  
-------------------------------------

In this example, the liveness probe uses the TCP handler to check port 8080 is open and responding. With this configuration, the kubelet will attempt to open a socket to your container on the specified port. If the liveness probe fails, the container will be restarted.

```
apiVersion: v1
kind: Pod
metadata:
  name: liveness
  labels:
    app: liveness-tcp
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness:0.1
    ports:
    - containerPort: 8080
    livenessProbe:
      tcpSocket:
        port: 8080
      initialDelaySeconds: 5
      periodSeconds: 5
```

Example 3: HTTPGetAction handler
--------------------------------

This example shows the HTTP handler, which will send an HTTP GET request on port 8080 to the /health path. If a code between 200–400 is returned, the probe is considered successful. If a code outside of this range is returned, the probe is unsuccessful, and the container is restarted. The httpHeaders option is used to define any custom headers you want to send.

```
apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: liveness-http
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/liveness:0.1
    livenessProbe:
      httpGet:
        path: /health
        port: 8080
        httpHeaders:
        - name: Custom-Header
          value: ItsAlive
      initialDelaySeconds: 5
      periodSeconds: 5
```

# To check if a liveness probe is working in Kubernetes, follow these steps:

Check pod events – Run kubectl describe pod <pod-name> -n <namespace> and under the Events section for messages related to the liveness probe, such as failures or restarts.

Inspect pod logs – Check if the container is restarting due to probe failures with kubectl logs <pod-name> -n <namespace>. If the container restarts repeatedly, the liveness probe might be failing.

Check pod status – Run: kubectl get pods -n <namespace> . If the pod is in a CrashLoopBackOff or Restarting state, the liveness probe may be failing.

Describe deployment – For deployments, inspect the configuration: kubectl get deployment <deployment-name> -o yaml -n <namespace> . Look for the livenessProbe section under containers.

Manually test liveness probe endpoint – If the probe uses an HTTP endpoint, you can test it using curl: curl http://<pod-ip>:<liveness-port>/<probe-path>.


# READINESS PROBE
----------------------
- Readiness probes monitor when the application becomes available. 
- If it fails, no traffic will be sent to the Pod. They are used when an app needs configuration before it becomes ready. 
- If the readiness probe fails but the liveness probe succeeds, the kubelet determines that the container is not ready to receive network traffic but is still working to become ready.

Example-01
------------
In this example, the readiness probe is configured to run a custom command to run a script inside the container. Inside the container, the custom script should be responsible for performing the readiness check. The script can return a non-zero exit code if the readiness check fails and a zero exit code if it succeeds.

1. exec - specifies a Command check.
2. command  - is an array of commands to run inside the container. In this example, we run a shell script named “check-script.sh.”
3. initialDelaySeconds - specifies that the probe should start 20 seconds after the container starts.
4. periodSeconds - specifies that the probe will be repeated every 15 seconds after the initial delay.

```
apiVersion: v1
kind: Pod
metadata:
  name: my-app-pod
spec:
  containers:
  - name: my-app-container
    image: my-app-image
    ports:
    - containerPort: 80
    readinessProbe:
      exec:
        command:
        - /bin/sh
        - -c
        - check-script.sh
      initialDelaySeconds: 20
      periodSeconds: 15
```

# STARTUP PROBES
--------------------
- The kubelet uses startup probes to detect when a container application has started. 
- When these are configured, liveness and readiness checks are disabled until they are successful, ensuring startup probes don’t interfere with the application startup.
- These are particularly useful with slow-starting containers, preventing the kubelet from killing them before they are up and running when a liveness probe fails.
- If liveness probes are used on the same endpoint as a startup probe, set the failureThreshold of the startup probe higher to support long startup times.
- If it fails, the event is logged, and the kubelet kills the container according to the configured restartPolicy.
