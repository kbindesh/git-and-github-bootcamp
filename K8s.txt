Understanding pods
=====================
You can create a pod using an imperative command as follows:  
kubectl run <pod-name> --image=<image-name:image-tag>

kubectl run nginx-pod --image=nginx:alpine

kubectl describe pod nginx-pod
--------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
name: nginx
spec:
containers:
- name: nginx
image: nginx:alpine
ports:
- containerPort: 80

kubectl apply -f <your-spec>.yaml
--------------------------------------------------
Similarly, we can run a BusyBox image with a single command, such as the following:  
kubectl run busybox --rm -it --image=busybox /bin/sh

---------------------------------------------------
Understanding a multi-container pod
---------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
name: multi-app-pod
labels:
app: multi-app
spec:
containers:
- name: nginx
image: nginx
ports:
- containerPort: 80
- name: busybox-sidecar
image: busybox
command: ['sh', '-c', 'while true; do sleep 3600; done;']

---------------------------------------------------
Understanding an init container
---------------------------------------------------
An init container is configured in a pod to execute before the container host starts. It is specified
inside an initContainers section, as in the following example. You can configure multiple init
containers too, which will allow each init container to complete one at a time in sequential order:

apiVersion: v1
kind: Pod
metadata:
  name: melon-pod
  labels:
    app: melonapp
spec:
  containers:
  - name: melonapp-container
    image: busybox:latest
    command: ['sh', '-c', 'echo The melonapp is running! &&
    sleep 3600']
  initContainers:
  - name: init-melonservice
    image: busybox:latest
    command: ['sh', '-c', 'until nslookup melonservice; do echo
    waiting for melonservice; sleep 2; done;']

In the case that any of the init containers fail to complete, Kubernetes will restart the pod repeatedly until
the init container succeeds.

=====================================================

=====================================================
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/

# Pod Lifecycle phases

# Container Lifecycle phases

# Container restart policy 
=====================================================
Deploying and managing applications
=====================================================
# Replicasets
-------------
apiVersion: apps/v1
kind: ReplicaSet
metadata:
name: frontend
labels:
app: melonapp-rs
spec:
replicas: 3
selector:
matchLabels:
app: melonapp-rs
template:
metadata:
labels:
app: melonapp-rs
spec:
containers:
- name: nginx
image: nginx

kubectl get replicaset

Once the ReplicaSet is deployed, update the number of ReplicaSets by using the following command:
kubectl scale replicaset frontend --replicas=6

Alternatively, you can specify it in a YAML definition with the following command:
kubectl scale --replicas=6 -f replicas.yaml

kubectl delete replicaset frontend

=========================================
Workload scheduling
=========================================
Understanding namespaces

kubectl get namespaces

When you define a pod or any namespaced Kubernetes object, you can specify the namespace in the
YAML definition as follows:

apiVersion: v1
kind: Pod
metadata:
  name: k8s-ns-pod
  namespace: k8s-ns
  labels:
    app: k8sapp
spec:
  containers:
  - name: k8sapp-container
    image: busybox
    command: ['sh', '-c', 'echo Salut K8S! && sleep 3600']

# If you create that pod and specify the namespace that the pod belongs to, you can add the -n flag
when querying this pod using the kubectl get pods command:

kubectl get pods -n k8s-ns

# Similarly, if the pod has been created in that namespace, you can use the following command to
check it out:
kubectl describe pod k8s-ms-pod -n k8s-ns

# In the case that the pods are not in the default namespace, you don’t have to specify the namespace
option anymore. In the following example, you want to set a namespace named dev, and then use
the kubectl get command without the -n flag:

kubectl config set-context &(kubectl config current-context)
--namespace=dev

====================================================
Labels, node selectors, and annotations
====================================================
- Labels, selectors, and annotations are useful notions when it comes to workload scheduling. Labels are key-value pairs attached to Kubernetes objects that can be listed in the metadata.labels section of an object descriptor. 
- Selectors are used for identifying and selecting a group of objects using their labels. 

- See the following examples of some quality-based selectors:

kubectl get pods -l app=my-app
kubectl get pods -l environment=production

# When it comes to inequality, you can use the following:
kubectl get pods -l environment!=production

# You can start by labeling the worker nodes using the following command:
kubectl label node cloudmelonplayground env=dev

========================================================
SERVICES
========================================================
Kubernetes treats Pods as ephemeral objects and deletes them when any of the following
events occur:
• Scale-down operations
• Rolling updates
• Rollbacks
• Failures

This means they’re unreliable, and apps can’t rely on them being there to respond to
requests. Fortunately, Kubernetes has a solution — Service objects sit in front of one or
more identical Pods and expose them via a reliable DNS name, IP address, and port.

- Every Service has a front end and a back end. 
- The front end includes a DNS name, IP address, and network port that Kubernetes guarantees will never change. 
- The back end is a label selector that sends traffic to healthy Pods with matching labels.

Example for exmplaining service

apiVersion: apps/v1
kind: Deployment
metadata:
  name: bin-2024
spec:
  replicas: 10
  <Snip>
  template:
    metadata:
      labels:
      project: tkb <<==== Create Pods with these labels
      zone: prod <<==== Create Pods with these labels
    spec:
      containers:
  <Snip>
---
apiVersion: v1
kind: Service
metadata:
  name: tkb
spec:
  ports:
    - port: 8080
  selector:
    project: tkb <<==== Send to Pods with these labels
    zone: prod <<==== Send to Pods with these labels


==================================================================
Service types
Kubernetes has several types of Services for different use cases and requirements. The
major ones are:
• ClusterIP
• NodePort
• LoadBalancer

===================================================================
NodePort
===================================================================
apiVersion: v1
kind: Service
metadata:
  name: skippy <<==== Registered with the internal cluster DNS (ClusterIP)
spec:
  type: NodePort <<==== Service type
  ports:
  - port: 8080 <<==== ClusterIP port
    targetPort: 9000 <<==== Application port in container
    nodePort: 30050 <<==== External port on every cluster node (NodePort)
  selector:
    app: hello-world

kubectl describe svc svc-test
[Endpoints: 10.1.0.200:8080,10.1.0.201:8080,10.1.0.202:808]
====================================================================
Loadbalancer
====================================================================
apiVersion: v1
kind: Service
metadata:
  name: lb <<==== Registered with cluster DNS
spec:
  type: LoadBalancer
  ports:
  - port: 8080 <<==== Load balancer port
    targetPort: 9000 <<==== Application port inside container
  selector:
    project: tkb

kubectl describe svc svc-test
[Endpoints: 10.1.0.200:8080,10.1.0.201:8080,10.1.0.202:8080]


===================================================================
KUBERNETES SECURITY
===================================================================
kubectl get roles -A
kubectl get rolesbindings -A


https://kubernetes.io/docs/reference/access-authn-authz/rbac/

Kubernetes allows you to configure custom roles or use default user-facing roles, including, but not limited to:

1. Cluster-admin: 
 - This “superuser” can perform any action on any resource in a cluster. 
 - You can use this in a ClusterRoleBinding to grant full control over every resource in the cluster (and in all namespaces) or in a RoleBinding to grant full control over every resource in the respective namespace.
2. Admin: 
 - This role permits unlimited read/write access to resources within a namespace. 
 - This role can create roles and role bindings within a particular namespace. It does not permit write access to the namespace itself.
3. Edit: This role grants read/write access within a given Kubernetes namespace. 
   - It cannot view or modify roles or role bindings. 
4. View: This role allows read-only access within a given namespace. 
   - It does not allow viewing or modifying of roles or role bindings. 


# Creating roles for users per user type
===========================================
As an example, let’s say your organization needs roles for three different user types: 
1. infrastructure monitoring team members - you could configure a Role that gives read-only access (using the verbs “get,” “list” and “watch”) to a given namespace.
2. established developers - 
3. cluster admins - And admin could map new developers, who are still getting the hang of things, or anyone else who needs read-only access, to this Role.

# READ-ONLY ROLE for infrastructure monitoring users

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: read-only
  namespace: default
rules:
- apiGroups:
  - ""
  resources: ["*"]
  verbs:
  - get
  - list
  - watch

To apply the Role to a user, you must define a RoleBinding. 
This will give the user access to all resources within the namespace with the permissions defined in the Role configuration above:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: read-only-binding
roleRef:
  kind: Role
  name: read-only #The role name you defined in the Role configuration
  apiGroup: rbac.authorization.k8s.io
subjects:
– kind: User
  name: example #The name of the user to give the role to
  apiGroup: rbac.authorization.k8s.io


# READ-WRITE ROLE FOR DEVELOPERS

Before creating a role, create a dedicated namespace for developers "dev"

Likewise, a Role for developers who need not just the read-access rights from the example above, but also write-access, to a certain namespace (“dev” in this example) would look as follows:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
 name: read-write
 namespace: dev
rules:
- apiGroups:
 - ""
 resources: ["*"]
 verbs:
 - get
 - list
 - watch
 - create
 - update
 - patch
 - delete

The associated RoleBinding would look like:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
 name: read-write-binding
roleRef:
 kind: Role
 name: read-write
 apiGroup: rbac.authorization.k8s.io
subjects:
- kind: User
 name: example
 apiGroup: rbac.authorization.k8s.io


# SUPER-USER ROLE FOR ENTIRE CLUSTER (CLUSTER ROLE)
------------------------------------------------------
For superusers who need admin access to the entire cluster, change the "kind" value to "ClusterRole," which gives the user access to all resources within the cluster, instead of just one namespace:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: superuser
rules:
- apiGroups:
 - ""
 resources: ["*"]
 verbs:
 - get
 - list
 - watch
 - create
 - update
 - patch
 - delete

And in this case, we’ll create a ClusterRoleBinding instead of a RoleBinding:

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
 name: superuser-binding
roleRef:
 kind: ClusterRole
 name: superuser
 apiGroup: rbac.authorization.k8s.io
subjects:
- kind: User
 name: superuser
 apiGroup: rbac.authorization.k8s.io


======================================================
SERVICE ACCOUNTS
======================================================

# Create service accounts for each application

- Kubernetes uses service accounts to authenticate and authorize requests by pods to the Kubernetes API server. 
- Kubernetes automatically assigns newly created pods to the “default” service account in your cluster and all applications share this service account. 
- However, this configuration may not be desirable if, 
  for example, you are using some applications for development purposes, and want those applications to use a “dev” service account instead of a default one.

https://www.strongdm.com/blog/kubernetes-rbac-role-based-access-control
https://medium.com/rahasak/kubernetes-role-base-access-control-with-service-account-e4c65e3f25cc
https://kubernetes-tutorial.schoolofdevops.com/configuring_authentication_and_authorization/

